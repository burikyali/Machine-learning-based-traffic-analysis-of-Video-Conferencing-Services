{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10187335,"sourceType":"datasetVersion","datasetId":6293679},{"sourceId":10294020,"sourceType":"datasetVersion","datasetId":6371016},{"sourceId":10294090,"sourceType":"datasetVersion","datasetId":6371069},{"sourceId":10294532,"sourceType":"datasetVersion","datasetId":6371397},{"sourceId":10311233,"sourceType":"datasetVersion","datasetId":6383140},{"sourceId":10334802,"sourceType":"datasetVersion","datasetId":6399278},{"sourceId":214452,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":182813,"modelId":205021}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/icewallowcomeyuff/ml-video-conferencing-analysis?scriptVersionId=215658005\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Machine Learning Based Traffic Analysis of Video Conferencing Services in rural communities - Main","metadata":{}},{"cell_type":"markdown","source":"# ðŸ“š | Import Libraries","metadata":{"execution":{"iopub.status.busy":"2024-12-20T12:25:22.092243Z","iopub.execute_input":"2024-12-20T12:25:22.092668Z","iopub.status.idle":"2024-12-20T12:25:22.111796Z","shell.execute_reply.started":"2024-12-20T12:25:22.092632Z","shell.execute_reply":"2024-12-20T12:25:22.110642Z"}}},{"cell_type":"code","source":"import keras_cv\nimport keras\nfrom keras import ops\nimport tensorflow as tf\n\nimport cv2\nimport pandas as pd\nimport numpy as np\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nimport joblib\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\n\n\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Library Versions","metadata":{}},{"cell_type":"code","source":"print(\"TensorFlow:\", tf.__version__)\nprint(\"Keras:\", keras.__version__)\nprint(\"KerasCV:\", keras_cv.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ’¾ | Data Preparation","metadata":{}},{"cell_type":"markdown","source":"## Initial Inspection","metadata":{}},{"cell_type":"markdown","source":"### The Zoom480pRaw Dataset contains the following QoS metrics :\n\n#### 1. Local time : The timestamp indicating when the data was captured.\n#### 2. Download (Rx) bandwidth (bps) : The bandwidth available for downloading data, measured in bits per second (bps).\n#### 3. Upload (Tx) bandwidth (bps) : The bandwidth available for uploading data, measured in bits per second (bps).\n#### 4. Rx packet loss (percent) : Percentage of packets lost during download.\n#### 5. Tx packet loss (percent) : Percentage of packets lost during upload.\n#### 6. RTT (ping) (ms : Round Trip Time (RTT) measures the time (in milliseconds) for a signal to travel to the server and back.\n#### 7. Rx instant jitter (ms) : Instantaneous jitter during download, measured in millisecond\n#### 8. Tx instant jitter (ms) : Instantaneous jitter during upload, measured in milliseconds.\n#### 9. Rx RFC3550 jitter (ms) : Jitter measured according to the RFC3550 standard for downloading packets.\n#### 10.Tx RFC3550 jitter (ms) : Jitter measured according to the RFC3550 standard for uploading packets\n#### 11.Rx packet loss burst length (packets) : The number of consecutive packets lost during download.\n#### 12.Tx packet loss burst length (packets) : The number of consecutive packets lost during upload.\n\n\n#### 13.Download bandwidth &\n#### 14.Upload Bandwidth : These columns represent textual descriptions of bandwidth values in a human-readable format, such as 2.08 bps/4.00 bps.","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\nfile_path = '/kaggle/input/zoom480p/zoom480pRaw.csv'  # Update this path based on your Kaggle dataset location\ndata = pd.read_csv(file_path)\n\n# Display the first few rows of the dataset\nprint(\"First few rows of the dataset:\")\ndisplay(data.head())\n\n# Display basic information about the dataset\nprint(\"\\nDataset Info:\")\ndata.info()\n\n# Check for missing values\nprint(\"\\nMissing Values per Column:\")\nprint(data.isnull().sum())\n\n# Display summary statistics\nprint(\"\\nSummary Statistics:\")\ndisplay(data.describe())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Redundancy Checking between Rx and Tx metrics.\n","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculate correlations between Rx and Tx metrics\nrx_tx_correlations = data[[\n    'Rx packet loss (percent)', 'Tx packet loss (percent)',\n    'Rx instant jitter (ms)', 'Tx instant jitter (ms)'\n]].corr()\n\n# Display the correlation matrix\nprint(\"\\nCorrelation Matrix for Rx and Tx Metrics:\")\nprint(rx_tx_correlations)\n\n# Visualize the correlation matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(rx_tx_correlations, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Between Rx and Tx Metrics')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The low correlation values indicate that Rx and Tx metrics do not show significant redundancy. This suggests that both metrics capture different aspects of the network's QoS and should be retained. There is no strong redundancy between Rx and Tx jitter or packet loss. Hence, both sets of metrics can provide unique value for analyzing the dataset.","metadata":{}},{"cell_type":"markdown","source":"## Redundancy checking between RFC3550 jitter (ms) & Instant Jitter (ms)","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Select jitter columns for correlation analysis\njitter_columns = [\n    'Rx instant jitter (ms)', 'Tx instant jitter (ms)', \n    'Rx RFC3550 jitter (ms)', 'Tx RFC3550 jitter (ms)'\n]\n\n# Calculate correlation matrix\njitter_correlation = data[jitter_columns].corr()\n\n# Display the correlation matrix\nprint(\"Correlation Matrix for Jitter Metrics:\")\nprint(jitter_correlation)\n\n# Visualize the correlation matrix using a heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(jitter_correlation, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title(\"Correlation Between Instant Jitter and RFC3550 Jitter\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### We can see high values (e.g., 0.8 or above) between:\n\n### * Rx instant jitter (ms) and Rx RFC3550 jitter (ms).\n### * Tx instant jitter (ms) and Tx RFC3550 jitter (ms).\n\n### Since instant jitter is more intuitive and directly reflects real-time conditions, keeping it and dropping RFC3550 jitter simplifies the model without losing much information.","metadata":{}},{"cell_type":"markdown","source":"## Cleaning & Extracting Necessary Features","metadata":{}},{"cell_type":"markdown","source":"### Column selection retained the most relevant features, reducing unnecessary noise in the dataset.\n\n### Columns Dropped:\n\n### 1. Download bandwidth: Redundant, as Download (Rx) bandwidth (bps) provides the same information in numeric format.\n### 2. Upload bandwidth:   Redundant, as Upload (Tx) bandwidth (bps) provides the same information in numeric format.\n\n### 3. Rx RFC3550 jitter (ms)\n### 4. Tx RFC3550 jitter (ms) \n\n### The dataset already includes Rx instant jitter (ms) and Tx instant jitter (ms), which capture jitter values for incoming and outgoing packets.The RFC3550 jitter columns represent a standardized method to compute jitter, but in practice, they are often highly correlated with the \"instant jitter\" columns.\n\n### 5. Rx packet loss burst length (packets)\n### 6. Tx packet loss burst length (packets)\n\n### These columns measure the length of consecutive packets lost during download (Rx) or upload (Tx).\n### While they provide detailed insights into packet loss events, their impact is already captured by Rx packet loss (percent) and Tx packet loss (percent): For example, if 5 packets are lost consecutively, it would already contribute to the total packet loss percentage.\n### We chose Packet Loss Percent over the Number of Packets Dropped because it is normalized, allowing for easy comparison across datasets and services, regardless of the total packet count. Additionally, it is more directly correlated with QoE and easier for machine learning models to interpret and scale.","metadata":{}},{"cell_type":"code","source":"# Select necessary numeric columns for analysis\ncolumns_to_keep = [\n    \"Local time\", \n    \"Download (Rx) bandwidth (bps)\", \n    \"Upload (Tx) bandwidth (bps)\",\n    \"Rx packet loss (percent)\", \n    \"Tx packet loss (percent)\", \n    \"RTT (ping) (ms)\",\n    \"Rx instant jitter (ms)\", \n    \"Tx instant jitter (ms)\"\n]\n\n# Subset the dataset\ndata = data[columns_to_keep]\n\n# Display the cleaned subset\nprint(\"Selected Columns:\")\ndisplay(data.head())\n\n# Check for any missing or invalid values in the selected columns\nprint(\"\\nMissing Values After Column Selection:\")\nprint(data.isnull().sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Mean Jitter & Jitter Difference","metadata":{}},{"cell_type":"markdown","source":"###  **Mean Jitter**\n\n* ### QoE is often affected by the overall jitter in the network, not just Rx (receive) or Tx (transmit) jitter individually.\n* ### Mean Jitter provides a single value summarizing the average network instability, representing the combined impact of incoming and outgoing jitter.\n\n### **Jitter Difference**\n\n* ### When thereâ€™s a significant difference between Rx and Tx jitter, it often indicates imbalanced network conditions (e.g., poor uplink but stable downlink).\n* ### A large Jitter Difference value helps pinpoint whether the bottleneck is on the upload (Tx) or download (Rx) side of the network.\n","metadata":{}},{"cell_type":"code","source":"data['Mean Jitter'] = (data['Rx instant jitter (ms)'] + data['Tx instant jitter (ms)']) / 2\n\ndata['Jitter Difference'] = data['Rx instant jitter (ms)'] - data['Tx instant jitter (ms)']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Unit Consistency","metadata":{}},{"cell_type":"markdown","source":"###  Ensures all numeric metrics are in comparable units. bps (bandwidth) was converted to Mbps for easier interpretation.","metadata":{}},{"cell_type":"code","source":"# Convert bandwidth metrics from bps to Mbps\ndata['Download (Rx) bandwidth (Mbps)'] = data['Download (Rx) bandwidth (bps)'] / 1e6\ndata['Upload (Tx) bandwidth (Mbps)'] = data['Upload (Tx) bandwidth (bps)'] / 1e6\n\n# Drop the original bandwidth columns (in bps) to avoid confusion\ndata.drop(columns=['Download (Rx) bandwidth (bps)', 'Upload (Tx) bandwidth (bps)'], inplace=True)\n\n# Display the updated dataset\nprint(\"\\nDataset After Unit Conversion:\")\ndisplay(data.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Standardizing the Timestamp","metadata":{}},{"cell_type":"markdown","source":"### Timestamps in the raw dataset might be in string format, making it difficult to perform time-based calculations or analysis. Standardization converts them to a machine-readable datetime format.","metadata":{}},{"cell_type":"code","source":"# Convert \"Local time\" to datetime format\ndata['Local time'] = pd.to_datetime(data['Local time'], errors='coerce')\n\n# Check for invalid timestamps\nprint(\"\\nRows with Invalid Timestamps:\")\nprint(data[data['Local time'].isnull()])\n\n# Remove rows with invalid timestamps\ndata = data.dropna(subset=['Local time'])\n\n# Sort the data by timestamp (if not already sorted)\ndata = data.sort_values(by='Local time')\n\n# Display the standardized timestamps\nprint(\"\\nData After Standardizing Timestamps:\")\ndisplay(data.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Adding Elapsed Time Feature","metadata":{}},{"cell_type":"markdown","source":"### To introduce a temporal dimension, capturing how QoS metrics change over time. Helps identify whether network conditions improve or degrade over time.","metadata":{}},{"cell_type":"code","source":"# Calculate elapsed time in seconds from the first timestamp\ndata['Elapsed Time'] = (data['Local time'] - data['Local time'].iloc[0]).dt.total_seconds()\n\n# Display the dataset with the new \"Elapsed Time\" column\nprint(\"\\nDataset with Elapsed Time:\")\ndisplay(data.head(100))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Handling Missing & Invalid Data","metadata":{}},{"cell_type":"code","source":"# Check for missing values in numeric columns\nnumeric_columns = data.select_dtypes(include=[np.number]).columns\nprint(\"\\nMissing Values in Numeric Columns Before Imputation:\")\nprint(data[numeric_columns].isnull().sum())\n\n# Impute missing values with the mean of each column\ndata[numeric_columns] = data[numeric_columns].fillna(data[numeric_columns].mean())\n\n# Verify missing values are handled\nprint(\"\\nMissing Values After Imputation:\")\nprint(data[numeric_columns].isnull().sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the current prepared data","metadata":{}},{"cell_type":"code","source":"print(\"\\nDataset after anaylsis & preparation:\")\ndisplay(data.head(100))\n\n# After handling missing and invalid data\ndata['Service'] = 'Zoom'\n\n\n# Save the cleaned and preprocessed data for downstream tasks\noutput_path = './zoom480p_preprocessed.csv'\ndata.to_csv(output_path, index=False)\n\nprint(\"\\nPreprocessed dataset saved as 'zoom480p_preprocessed.csv'.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸ“º | Services Comparison (Google Meet , Microsoft Teams, Zoom)","metadata":{}},{"cell_type":"markdown","source":"## Loading & Inspecting Pre-processed Datasets","metadata":{}},{"cell_type":"code","source":"# Load preprocessed datasets\ngmeet_data = pd.read_csv('/kaggle/input/preprocessed-datasets/gmeet480p_preprocessed.csv')\nteams_data = pd.read_csv('/kaggle/input/preprocessed-datasets/teams480p_preprocessed.csv')\nzoom_data = pd.read_csv('/kaggle/input/preprocessed-datasets/zoom480p_preprocessed.csv')\n\n# Display dataset info\nprint(\"Gmeet Dataset Info:\")\nprint(gmeet_data.info(), \"\\n\")\nprint(\"Teams Dataset Info:\")\nprint(teams_data.info(), \"\\n\")\nprint(\"Zoom Dataset Info:\")\nprint(zoom_data.info(), \"\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Comparing & Visualizing Key QoS Metrics","metadata":{}},{"cell_type":"markdown","source":"### This code is used to compare and visualize the distributions of key QoS metrics.","metadata":{}},{"cell_type":"code","source":"metrics = ['Rx packet loss (percent)', 'Tx packet loss (percent)', \n           'RTT (ping) (ms)', 'Rx instant jitter (ms)', \n           'Download (Rx) bandwidth (Mbps)']\n\n# Visualize each metric for all services\ndatasets = {'Gmeet': gmeet_data, 'Teams': teams_data, 'Zoom': zoom_data}\nfor metric in metrics:\n    plt.figure(figsize=(10, 6))\n    # Combine datasets for the current metric with a 'Service' column\n    combined_data = pd.concat([\n        data[[metric]].assign(Service=service) for service, data in datasets.items()\n    ], ignore_index=True)\n    # Create boxplot\n    sns.boxplot(data=combined_data, x='Service', y=metric, palette='Set3')\n    plt.title(f'{metric} Distribution Across Services')\n    plt.ylabel(metric)\n    plt.xlabel('Service')\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### From the output:\n\n\n* ### Gmeet shows consistent performance but with high outliers in RTT and jitter.\n* ### Teams demonstrates variability in packet loss and stable bandwidth.\n* ### Zoom maintains low outliers and balanced performance across metrics.\n","metadata":{}},{"cell_type":"markdown","source":"## Assessing Variability and Stability","metadata":{}},{"cell_type":"markdown","source":"### Variability of the dataset calculated by Standard Deviation.\n###  Higher standard deviation indicates more variability in QoS metrics, which could lead to inconsistent QoE.","metadata":{}},{"cell_type":"code","source":"# Compute standard deviation for QoS metrics\nfor service, data in datasets.items():\n    print(f\"Standard Deviation for {service}:\")\n    display(data[metrics].std())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 1.  Google Meet\n\n\n* ### High variability is observed in Tx packet loss (32.22%), RTT (155.07 ms), and Rx instant jitter (55.29 ms), indicating inconsistent performance.\n* ### The bandwidth (0.49 Mbps) shows lower variability, suggesting more stable download speeds.\\\n* ### The high standard deviation in packet loss and RTT indicates QoS inconsistencies affecting user experience.\n\n### 2. Microsoft Teams\n\n\n* ### Low variability is observed in Tx packet loss (0.01%) and Rx instant jitter (11.09 ms), suggesting consistent performance in these metrics.\n* ### RTT (13.85 ms) and Rx packet loss (0.78%) have moderate variability, indicating some instability.\n* ### The low standard deviations indicate better overall stability compared to Gmeet and Zoom.\n\n### 3. Zoom:\n\n\n* ### Moderate variability is observed in Rx packet loss (0.65%), RTT (17.52 ms), and Rx instant jitter (11.65 ms), indicating relatively consistent performance.\n* ### Bandwidth variability is low (0.19 Mbps), which helps maintain steady download speeds.\n* ### The moderate standard deviations indicate a balance between stability and variability.\n\n","metadata":{}},{"cell_type":"markdown","source":"### Here, we Identify outliers in QoS metrics using the 1.5 Ã— IQR rule for each service and metric.\n\n### Outliers represent extreme values that deviate significantly from the typical range, potentially indicating extremely poor network performance with respect to our dataset.","metadata":{}},{"cell_type":"code","source":"# Identify outliers for each metric using 1.5 * IQR rule\nfor service, data in datasets.items():\n    print(f\"Outliers for {service}:\")\n    for metric in metrics:\n        q1 = data[metric].quantile(0.25)\n        q3 = data[metric].quantile(0.75)\n        iqr = q3 - q1\n        outliers = data[(data[metric] < (q1 - 1.5 * iqr)) | (data[metric] > (q3 + 1.5 * iqr))]\n        print(f\"{metric}: {len(outliers)} outliers\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### 1. Gmeet: High outliers in packet loss, jitter, and bandwidth indicate unstable performance under extreme conditions.\n### 2. Teams: Few outliers across metrics show consistent performance, with no bandwidth-related outliers.\n### 3. Zoom: Significant outliers in Tx packet loss and moderate in jitter and RTT indicate occasional performance degradation.\n","metadata":{}},{"cell_type":"markdown","source":"### Insights\n* ### Teams: Low variability and few outliers make it suitable for modeling stable QoE.\n* ### Gmeet: High variability and outliers provide insights into extreme QoS cases.\n* ### Zoom: Balanced variability and outliers make it a good mix of stable and unstable scenarios.\n","metadata":{}},{"cell_type":"markdown","source":"## Aggregating Performance Scores of Services (Gmeet, Teams, Zoom)","metadata":{}},{"cell_type":"markdown","source":"### The scores for each service (Gmeet, Teams, Zoom) were calculated to evaluate their performance based on key QoS metrics.\n\n### Metrics Considered:\n\n\n* ### Rx packet loss (percent): Measures packet loss during download (lower is better).\n* ### Tx packet loss (percent): Measures packet loss during upload (lower is better).\n* ### RTT (ping) (ms): Measures network latency (lower is better).### Rx instant jitter (ms): Measures variability in packet arrival times (lower is better).\n* ### Download (Rx) bandwidth (Mbps): Measures network capacity (higher is better).\n\n\n### Score Calculation Steps:\n\n\n* ### Normalization: QoS metrics were normalized using Min-Max scaling to bring all values to a comparable range (0 to 1).\n* ### Inversion: Normalized values were inverted (1 - normalized) so that better QoS values correspond to higher scores.\n* ### Weighted Scoring: Scores were calculated using matrix multiplication with predefined weights for each metric.\n","metadata":{}},{"cell_type":"markdown","source":"### Weights were derived based on their relative impact on QoE, as identified in the research paper \"Analysis of the Effect of QoS on Video Conferencing QoE\".\n\n\n1. ### Packet Loss (Rx/Tx): Most significant factors affecting QoE due to their direct impact on audio/video quality. Weight = 0.6 (0.3 each).\n2. ### RTT (Latency): Affects responsiveness and synchronization during calls. Weight = 0.2.\n3. ### Jitter: Causes frame drops and desynchronization. Weight = 0.1.\n4. ### Bandwidth: Ensures sufficient capacity for high-quality video but is less critical under stable conditions. Weight = 0.1.\n","metadata":{}},{"cell_type":"code","source":"metrics = ['Rx packet loss (percent)', 'Tx packet loss (percent)', \n           'RTT (ping) (ms)', 'Rx instant jitter (ms)', \n           'Download (Rx) bandwidth (Mbps)']\n\nweights = {\n    'Rx packet loss (percent)': 0.25,\n    'Tx packet loss (percent)': 0.25,\n    'RTT (ping) (ms)': 0.1,\n    'Rx instant jitter (ms)': 0.3,\n    'Download (Rx) bandwidth (Mbps)': 0.1\n}\n\n# Function to calculate scores\ndef calculate_scores(data, metrics, weights):\n    # Normalize metrics\n    scaler = MinMaxScaler()\n    normalized = scaler.fit_transform(data[metrics])\n    \n    # Invert negative QoS metrics (lower is better)\n    inverted = 1 - normalized  # Apply inversion for all metrics\n    \n    # Correct the inversion for bandwidth (higher is better)\n    # Bandwidth is assumed to be the last metric in the list (adjust as necessary)\n    inverted[:, metrics.index('Download (Rx) bandwidth (Mbps)')] = normalized[:, metrics.index('Download (Rx) bandwidth (Mbps)')]\n    \n    # Compute weighted score\n    scores = inverted @ pd.Series(weights).values  # Matrix multiplication for scores\n    return scores\n\n# Re-run the score calculations with the updated function\ngmeet_scores = calculate_scores(gmeet_data, metrics, weights)\nteams_scores = calculate_scores(teams_data, metrics, weights)\nzoom_scores = calculate_scores(zoom_data, metrics, weights)\n\n# Add scores back to the datasets for analysis\ngmeet_data['Score'] = gmeet_scores\nteams_data['Score'] = teams_scores\nzoom_data['Score'] = zoom_scores\n\n# Calculate average scores for each service\naverage_scores = {\n    'Gmeet': gmeet_data['Score'].mean(),\n    'Teams': teams_data['Score'].mean(),\n    'Zoom': zoom_data['Score'].mean()\n}\n\nprint(\"Average Scores by Service:\")\nfor service, score in average_scores.items():\n    print(f\"{service}: {score}\")\n\n# Plot average scores for each service\nplt.figure(figsize=(10, 6))\nplt.bar(average_scores.keys(), average_scores.values(), color=['skyblue', 'lightgreen', 'salmon'])\nplt.title('Average Performance Scores by Service')\nplt.ylabel('Average Score')\nplt.xlabel('Service')\nplt.ylim(0, max(average_scores.values()) + 0.1)  # Set y-limit slightly above the max score for better visualization\nfor i, score in enumerate(average_scores.values()):\n    plt.text(i, score + 0.01, f\"{score:.2f}\", ha='center', fontsize=10)  # Add score labels on the bars\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The bar chart shows the average performance scores for Gmeet, Teams, and Zoom, highlighting Zoom as the best-performing service with a score of 0.90, followed by Teams (0.86) and Gmeet (0.83). This ranking reflects the combined weighted impact of key QoS metrics.","metadata":{}},{"cell_type":"markdown","source":"## Evaluating Dataset Suitability","metadata":{}},{"cell_type":"markdown","source":"### Dataset Diversity","metadata":{}},{"cell_type":"markdown","source":"### The code computes the diversity of QoS metrics (e.g., packet loss, RTT, jitter, bandwidth) for each service by calculating the range (difference between maximum and minimum values) of each metric.\n\n\n* ### For each service, the diversity for each QoS metric is computed as: **Diversity = MaxÂ Value âˆ’ MinÂ Value**\n* ### A larger range indicates greater diversity in the dataset for that metric.\n\n\n### A dataset with higher diversity captures a broader range of network conditions, making it better suited for training a machine learning model that can generalize well. Low diversity may lead to models that overfit specific conditions and perform poorly in real-world scenarios.","metadata":{}},{"cell_type":"code","source":"# Calculate diversity in QoS metrics for each service using range\ndiversity_data = {}\nfor service, data in datasets.items():\n    diversity = data[metrics].max() - data[metrics].min()\n    diversity_data[service] = diversity\n\n# Convert to DataFrame for visualization\ndiversity_df = pd.DataFrame(diversity_data)\n\n# Plot diversity for each metric\ndiversity_df.plot(kind='bar', figsize=(12, 8), title='Diversity in QoS Metrics Across Services')\nplt.ylabel('Range of Values')\nplt.xticks(rotation=45)\nplt.legend(title=\"Service\", loc='upper right')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  Gmeet has the highest variability in critical metrics like RTT and Rx jitter. This indicates Gmeetâ€™s dataset covers a broader range of conditions, making it valuable for training robust models that can generalize well.","metadata":{}},{"cell_type":"markdown","source":"### Dataset Size","metadata":{}},{"cell_type":"markdown","source":"### Analyzing the Dataset Size Across the 3 Services","metadata":{}},{"cell_type":"code","source":"# Calculate dataset size for each service\ndataset_sizes = {service: len(data) for service, data in datasets.items()}\n\n# Plot dataset sizes\nplt.figure(figsize=(10, 6))\nplt.bar(dataset_sizes.keys(), dataset_sizes.values(), color='skyblue')\nplt.title('Dataset Size Comparison')\nplt.ylabel('Number of Rows')\nplt.xlabel('Service')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The bar chart compares dataset sizes, showing Zoom with the largest number of rows, followed by Gmeet, and then Teams. Larger dataset sizes, like Zoom's, ensure better model training and generalization by providing more data points for learning patterns.","metadata":{}},{"cell_type":"markdown","source":"### Distribution of Challenging/Poor QoS Conditons","metadata":{}},{"cell_type":"markdown","source":"### Thresholds Defined: Specific thresholds were established for key QoS metrics, such as jitter > 30 ms, packet loss > 1%, and RTT > 150 ms, based on the research paper \"Analysis of the Effect of QoS on Video Conferencing QoE\" \n\n\n### 1. For each metric and service, rows where the metric exceeds the threshold were identified and counted.\n### 2. The count of challenging rows was divided by the total number of rows in the dataset, expressed as a percentage.\n\n\n\n\n* ### This analysis helps determine which dataset contains a higher proportion of challenging conditions, making it more suitable for training models to handle poor network scenarios.\n* ### Training models on datasets with a higher proportion of challenging conditions can improve their robustness and performance in real-world applications where such conditions are common.\n","metadata":{}},{"cell_type":"code","source":"# Define thresholds for challenging conditions\nthresholds = {\n    'Rx instant jitter (ms)': 30,\n    'Tx instant jitter (ms)': 30,\n    'Rx packet loss (percent)': 1,\n    'Tx packet loss (percent)': 1,\n    'RTT (ping) (ms)': 150\n}\n\n# Calculate proportion of challenging conditions\nchallenging_data = {}\nfor service, data in datasets.items():\n    proportions = {}\n    for metric, threshold in thresholds.items():\n        challenging_rows = len(data[data[metric] > threshold])\n        proportions[metric] = challenging_rows / len(data) * 100  # Percentage\n    challenging_data[service] = proportions\n\n# Convert to DataFrame for visualization\nchallenging_df = pd.DataFrame(challenging_data)\n\n# Plot challenging conditions for each metric\nchallenging_df.plot(kind='bar', figsize=(12, 8), title='Proportion of Challenging Conditions Across Services')\nplt.ylabel('Percentage of Rows (%)')\nplt.xticks(rotation=45)\nplt.legend(title=\"Service\", loc='upper right')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* ### Gmeet has the highest proportion of rows exceeding thresholds for Rx instant jitter, Tx instant jitter, and RTT, indicating frequent poor performance in these metrics.\n* ### Zoom exhibits significant challenging conditions in Tx instant jitter but performs better in packet loss metrics compared to Gmeet.\n* ### Teams shows the lowest proportion of challenging conditions across most metrics, particularly in jitter and packet loss, highlighting its stability under adverse conditions.\n\n\n### Gmeet's dataset captures more challenging scenarios, making it valuable for training models to handle poor network conditions.\n","metadata":{}},{"cell_type":"markdown","source":"## Visualizing Dataset Suitability","metadata":{}},{"cell_type":"markdown","source":"### Here, we evaluate the overall suitability of the datasets (Gmeet, Teams, Zoom) for machine learning model training by combining three key factors: diversity, size, and challenging conditions.\n\n\n### 1. Data Normalization: Z-Score Normalization is applied to ensure that all components are on the same scale, avoiding bias due to differing ranges in raw values.\n\n* #### The raw values of the criteria (Diversity, Size, and Challenging Conditions) have different ranges and units.\n* #### Z-Score Normalization standardizes these values by centering them around a mean of 0 with a standard deviation of 1, making them directly comparable.\n* #### Min-Max normalization scales values between 0 and 1, but it is sensitive to outliers. If one dataset has extreme values in one criterion (e.g., high Diversity), it could dominate the normalized scores, skewing the final rankings.\n\n### 2. Weights are assigned to each criterion:\n   * * #### Diversity (50%): Captures network variability for better model generalization.\n   * * #### Size (40%): Ensures sufficient data for robust training.\n   * * #### Challenging Conditions (10%): Focuses on poor QoS scenarios for model robustness.\n### 3.  A weighted final score is calculated for each dataset.\n\n\n### Datasets are ranked based on their final weighted scores, with the highest-scoring dataset being the most suitable for further analysis & training \n","metadata":{}},{"cell_type":"code","source":"# Define the components of suitability\nsuitability_ranking = {\n    'Diversity': diversity_df.mean(axis=0),  # Average diversity across all metrics\n    'Size': pd.Series(dataset_sizes),       # Dataset size\n    'Challenging Conditions': challenging_df.mean(axis=0)  # Average % of challenging conditions\n}\n\n# Combine into a DataFrame\nsuitability_df = pd.DataFrame(suitability_ranking)\n\n# Ensure no missing values\nsuitability_df = suitability_df.fillna(0)\n\n# Apply Z-Score Normalization\nscaler = StandardScaler()\nzscore_normalized_suitability = scaler.fit_transform(suitability_df)\nsuitability_df_normalized = pd.DataFrame(zscore_normalized_suitability, columns=suitability_df.columns, index=suitability_df.index)\n\n# Define weights for each criterion\ncriteria_weights = {\n    'Diversity': 0.5,\n    'Size': 0.4,\n    'Challenging Conditions': 0.1\n}\n\n# Calculate weighted final scores\nsuitability_df_normalized['Final Score'] = (\n    suitability_df_normalized['Diversity'] * criteria_weights['Diversity'] +\n    suitability_df_normalized['Size'] * criteria_weights['Size'] +\n    suitability_df_normalized['Challenging Conditions'] * criteria_weights['Challenging Conditions']\n)\n\n# Sort by final score to determine the best dataset\nsuitability_df_normalized = suitability_df_normalized.sort_values('Final Score', ascending=False)\n\n# Display rankings\nprint(\"Dataset Suitability Rankings (Weighted - Z-Score):\")\ndisplay(suitability_df_normalized)\n\n# Plot final scores\nplt.figure(figsize=(10, 6))\nplt.bar(suitability_df_normalized.index, suitability_df_normalized['Final Score'], color='skyblue')\nplt.title('Final Suitability Score for Each Dataset (Z-Score Normalization)')\nplt.ylabel('Weighted Final Score')\nplt.xlabel('Service')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Gmeet has the highest final score (1.04), indicating it is the most suitable dataset for model training & further analysis due to its high diversity and significant coverage of challenging conditions.\n### Teams has the lowest score (-0.99), primarily due to its small dataset size and low diversity.","metadata":{}},{"cell_type":"markdown","source":"# ðŸ“ˆ| Exploratory Data Analysis (EDA)(Google Meet)","metadata":{}},{"cell_type":"markdown","source":"## Inspecting the Dataset","metadata":{}},{"cell_type":"code","source":"# Load the dataset\ngmeet_data = pd.read_csv('/kaggle/input/preprocessed-datasets/gmeet480p_preprocessed.csv')\n\n# Display basic information and statistics\nprint(\"Dataset Info:\")\ngmeet_data.info()\n\nprint(\"\\nBasic Statistics:\")\ndisplay(gmeet_data.describe())\n\n# Check for missing values\nprint(\"\\nMissing Values:\")\ndisplay(gmeet_data.isnull().sum())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing Feature Distributions","metadata":{}},{"cell_type":"markdown","source":"### Histograms and Kernel Density Plots","metadata":{}},{"cell_type":"code","source":"# Plot histograms and KDE for QoS metrics\nmetrics = ['Rx packet loss (percent)', 'Tx packet loss (percent)', \n           'RTT (ping) (ms)', 'Rx instant jitter (ms)', 'Tx instant jitter (ms)', \n           'Download (Rx) bandwidth (Mbps)', 'Upload (Tx) bandwidth (Mbps)']\n\nfor metric in metrics:\n    plt.figure(figsize=(10, 6))\n    sns.histplot(gmeet_data[metric], kde=True, bins=30, color='skyblue')\n    plt.title(f'{metric} - Distribution')\n    plt.xlabel(metric)\n    plt.ylabel('Frequency')\n    plt.show()\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Key Points\n\n### Upload (Tx) reliability is less stable than Download (Rx) , with frequent instances of higher packet loss.\n### Similiarly, Tx instant jitter & Upload (Tx) Bandwidth is more unstable & spread compared to its Rx counterparts.\n### This is in line with the notion of Tx conditions to be considered important in optimizing network conditions compared to the Rx. ","metadata":{}},{"cell_type":"markdown","source":"### Distributions of Mean Jitter & Jitter Difference","metadata":{}},{"cell_type":"code","source":"# Visualize distributions of mean jitter and jitter difference\njitter_metrics = ['Mean Jitter', 'Jitter Difference']\n\nfor metric in jitter_metrics:\n    plt.figure(figsize=(10, 6))\n    sns.histplot(gmeet_data[metric], kde=True, bins=30, color='skyblue')\n    plt.title(f'{metric} - Distribution')\n    plt.xlabel(metric)\n    plt.ylabel('Frequency')\n    plt.show()\n\n# Box plots\nfor metric in jitter_metrics:\n    plt.figure(figsize=(10, 6))\n    sns.boxplot(y=gmeet_data[metric], color='lightgreen')\n    plt.title(f'{metric} - Box Plot')\n    plt.ylabel(metric)\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* ### ITU-T Recommendation G.1010 specifies that jitter values exceeding 30 ms can cause noticeable quality degradation, with values above 50 ms considered poor for video conferencing.\n* ### The histogram and box plots indicate that majority of the mean jitter values lie outside the acceptable values.\n* ### Jitter Difference is calculated as Rx instant jitter - Tx instant jitter. Negative values indicate the transmission (Tx) side faces more jitter, while positive values show higher jitter on the reception (Rx) side.\n* ### Significant negative values suggest the bottleneck is on the Tx side.\n","metadata":{}},{"cell_type":"markdown","source":"## Correlation Analysis","metadata":{}},{"cell_type":"markdown","source":"### Here, we calculate the correlation matrix for the QoS metrics in the dataset, which measures the linear relationship between each pair of metrics. Values range from -1 (strong negative correlation) to +1 (strong positive correlation).","metadata":{}},{"cell_type":"code","source":"# Calculate correlation matrix\ncorrelation_matrix = gmeet_data[metrics].corr()\n\n# Plot heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Heatmap of QoS Metrics')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* ### Between Tx Packet Loss and Upload Bandwidth, a strong negative correlation (-0.70) exists, indicating that as upload bandwidth decreases, the percentage of Tx packet loss increases. This suggests poor upload bandwidth significantly affects transmission quality.\n* ###  Rx instant jitter and Tx instant jitter do not strongly correlate with other metrics (close to 0), showing they may independently contribute to QoE issues.\n* ### The correlation between RTT and Tx Packet Loss is moderate (0.48). This suggests that as transmission packet loss increases, network latency (RTT) also tends to increase.\n* ### The correlation between Tx Instant Jitter and Upload Bandwidth is mild (0.41). This implies that unstable or lower upload bandwidth contributes to higher jitter, impacting the smooth delivery of video and audio during conferencing.\n","metadata":{}},{"cell_type":"markdown","source":"## Time-Series Trends","metadata":{}},{"cell_type":"markdown","source":"### Bandwidth over Time","metadata":{}},{"cell_type":"markdown","source":"### Here, we plot the download bandwidth (Download (Rx) bandwidth) over time, visualizing how bandwidth varies throughout the captured session.","metadata":{}},{"cell_type":"code","source":"# Convert 'Local time' to datetime if available\nif 'Local time' in gmeet_data.columns:\n    gmeet_data['Local time'] = pd.to_datetime(gmeet_data['Local time'])\n\n# Plot bandwidth over time\nif 'Local time' in gmeet_data.columns:\n    plt.figure(figsize=(12, 6))\n    plt.plot(gmeet_data['Local time'], gmeet_data['Download (Rx) bandwidth (Mbps)'], color='blue', label='Bandwidth')\n    plt.title('Download Bandwidth Over Time')\n    plt.xlabel('Time')\n    plt.ylabel('Bandwidth (Mbps)')\n    plt.legend()\n    plt.show()\nelse:\n    print(\"Local time column not available for time-series plotting.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* ### The plot shows noticeable fluctuations in download bandwidth over time, with several drops below 1 Mbps.\n* ### Bandwidth drops below 2 Mbps, particularly during video conferencing, can degrade quality by causing buffering, pixelation, or delays.\n","metadata":{}},{"cell_type":"markdown","source":"### Jitter Over Time","metadata":{}},{"cell_type":"markdown","source":"### The code visualizes how the Rx instant jitter metric changes over time to assess network stability for received packets.","metadata":{}},{"cell_type":"code","source":"# Plot jitter over time\nif 'Local time' in gmeet_data.columns:\n    plt.figure(figsize=(12, 6))\n    plt.plot(gmeet_data['Local time'], gmeet_data['Rx instant jitter (ms)'], color='orange', label='Rx Jitter')\n    plt.title('Rx Jitter Over Time')\n    plt.xlabel('Time')\n    plt.ylabel('Jitter (ms)')\n    plt.legend()\n    plt.show()\nelse:\n    print(\"Local time column not available for time-series plotting.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* ### The jitter plot reveals frequent fluctuations with some significant spikes exceeding 700 ms, indicating intermittent network instability.\n* ### High jitter values, especially above the ITU-T G.1010 threshold of 30 ms for conversational applications, suggest potential disruptions in video quality, such as delays or audio-video desynchronization.\n","metadata":{}},{"cell_type":"markdown","source":"# ðŸŽ¯| Deriving Target Variable ","metadata":{}},{"cell_type":"markdown","source":"##  Threshold-Based Target Derivation","metadata":{}},{"cell_type":"markdown","source":"\n### Here, we derive an initial target variable (`Initial_QoE`) based on ITU-T thresholds for QoS metrics. ie. ITU-T G.1010 & ITU-T G.114\n\n### **Steps**:\n1. ### **Define Thresholds**:\n   - ### ITU-T Recommendations were used to define thresholds for key QoS metrics:\n     - ### **Packet Loss (%)**: Good (<1%), Fair (1â€“3%), Poor (>3%).\n     - ### **RTT (ms)**: Good (<150ms), Fair (150â€“300ms), Poor (>300ms).\n     - ### **Jitter (ms)**: Good (<30ms), Fair (30â€“50ms), Poor (>50ms).\n2. ### **Assign Individual Metric Labels**:\n   - ### Each metric is labeled as \"Good,\" \"Fair,\" or \"Poor\" based on these thresholds.\n3. ### **Aggregate Labels**:\n   - ### Labels across all metrics are aggregated using a majority voting system to derive an overall QoE label (`Initial_QoE`).\n","metadata":{}},{"cell_type":"markdown","source":"### Define Thresholds and Apply Categorization","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Load the dataset\ngmeet_data = pd.read_csv('/kaggle/input/preprocessed-datasets/gmeet480p_preprocessed.csv')\n\n# Define ITU-T thresholds for QoS metrics\nthresholds = {\n    'Rx packet loss (percent)': [1, 3],  # Good (<1%), Fair (1-3%), Poor (>3%)\n    'Tx packet loss (percent)': [1, 3],\n    'RTT (ping) (ms)': [150, 300],      # Good (<150ms), Fair (150-300ms), Poor (>300ms)\n    'Rx instant jitter (ms)': [30, 50], # Good (<30ms), Fair (30-50ms), Poor (>50ms)\n    'Tx instant jitter (ms)': [30, 50]\n}\n\n# Function to assign thresholds\ndef assign_labels(row, metric, threshold):\n    if row[metric] < threshold[0]:\n        return 'Good'\n    elif threshold[0] <= row[metric] <= threshold[1]:\n        return 'Fair'\n    else:\n        return 'Poor'\n\n# Apply thresholds to derive labels\nfor metric, threshold in thresholds.items():\n    gmeet_data[f'{metric}_label'] = gmeet_data.apply(assign_labels, axis=1, metric=metric, threshold=threshold)\n\n# Aggregate individual metric labels into an overall QoE label\ndef majority_vote(row):\n    labels = [row[f'{metric}_label'] for metric in thresholds.keys()]\n    return max(set(labels), key=labels.count)\n\ngmeet_data['Initial_QoE'] = gmeet_data.apply(majority_vote, axis=1)\n\n\n# View the dataset with categorized metrics\ngmeet_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check counts for threshold-based labels (Initial_QoE)\nprint(\"Counts for Threshold-Based Labels (Initial_QoE):\")\nthreshold_counts = gmeet_data['Initial_QoE'].value_counts()\nprint(threshold_counts)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Semi-Supervised Clustering","metadata":{}},{"cell_type":"markdown","source":"### Normalize Data and Initialize K-Means","metadata":{}},{"cell_type":"markdown","source":"### Here, we refine the target variable using K-Means clustering with initialization based on the threshold-derived labels (`Initial_QoE`).\n\n### **Steps**:\n### 1. - Standardized the features using `StandardScaler` to ensure equal contribution in clustering.\n### 2. **Compute Initial Cluster Centers**:\n   - ### Calculated median QoS values for each `Initial_QoE` category (Good, Fair, Poor) to initialize K-Means cluster centers.\n### 3. **Apply K-Means**:\n   - ### Clustering with 3 clusters (`Good`, `Fair`, `Poor`) using the initialized cluster centers.\n### 4. **Map Clusters to QoE Labels**:\n   - ### Mapped the numeric cluster labels to string equivalents (Good, Fair, Poor) for interpretability.\n### 5. **Update Dataset**:\n   - ### Added new columns:\n     - ### `Cluster_QoE`: Numeric cluster labels.\n     - ### `Cluster_QoE_mapped`: String-mapped QoE labels.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\n# Select QoS metrics for clustering\nqos_features = ['Rx packet loss (percent)', 'Tx packet loss (percent)', \n                'RTT (ping) (ms)', 'Rx instant jitter (ms)', 'Tx instant jitter (ms)']\n\n# Standardize QoS features\nscaler = StandardScaler()\nqos_scaled = scaler.fit_transform(gmeet_data[qos_features])\n\n# Compute initial cluster centers based on threshold-based labels\ninitial_centers = gmeet_data.groupby('Initial_QoE')[qos_features].mean()\n\n# K-Means clustering with initialized cluster centers\nkmeans = KMeans(n_clusters=3, init=initial_centers.values, n_init=1)\ngmeet_data['Cluster_QoE'] = kmeans.fit_predict(qos_scaled)\n\n# Map cluster labels to string equivalents\ncluster_label_mapping = {0: 'Good', 1: 'Fair', 2: 'Poor'}  # Adjust based on your clustering results\ngmeet_data['Cluster_QoE_mapped'] = gmeet_data['Cluster_QoE'].map(cluster_label_mapping)\n\ngmeet_data.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Counts of Threshold-Based Labels & Clustering-Based Labels","metadata":{}},{"cell_type":"code","source":"# Check counts for threshold-based labels (Initial_QoE)\nprint(\"Counts for Threshold-Based Labels (Initial_QoE):\")\nthreshold_counts = gmeet_data['Initial_QoE'].value_counts()\nprint(threshold_counts)\n\n# Check counts for clustering-based labels (Cluster_QoE_mapped)\nprint(\"\\nCounts for Clustering-Based Labels (Cluster_QoE_mapped):\")\nclustering_counts = gmeet_data['Cluster_QoE_mapped'].value_counts()\nprint(clustering_counts)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### No. Of Mismatches between both","metadata":{}},{"cell_type":"code","source":"# Compare Initial_QoE and Cluster_QoE_mapped\nmismatches = gmeet_data[gmeet_data['Initial_QoE'] != gmeet_data['Cluster_QoE_mapped']]\nprint(f\"Number of mismatches: {len(mismatches)}\")\nprint(mismatches[['Initial_QoE', 'Cluster_QoE_mapped']].head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###  The similarity in counts between the threshold-based (Initial_QoE) and clustering-based (Cluster_QoE_mapped) labels but the low Cohen's Kappa Score indicates that the two methods are not assigning the same labels to individual data points consistently, even though their overall class distributions are comparable.","metadata":{}},{"cell_type":"markdown","source":"## Evaluate and Compare Methods","metadata":{}},{"cell_type":"markdown","source":"### Here, we :\n\n* ### Encode Final_QoE (mapped to numeric values).\n  \n* ### The clustering-based labels (Cluster_QoE) are evaluated using:\n    * ### Silhouette Score: Measures how well-defined the clusters are; closer to 1 is better.\n\n    * ### Davies-Bouldin Index: Evaluates cluster separation; lower values are better.\n\n    * ### Calinski-Harabasz Index: Measures compactness and separation; higher values are better.\n \n* ### The Feature-Target Correlation calculates the relationship between QoS metrics (qos_features) and the encoded target variable (Final_QoE_numeric).\n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, confusion_matrix, ConfusionMatrixDisplay\n\n# Encode all QoE labels to numeric values\nqoe_mapping = {'Good': 2, 'Fair': 1, 'Poor': 0}\ngmeet_data['Initial_QoE_numeric'] = gmeet_data['Initial_QoE'].map(qoe_mapping)\ngmeet_data['Cluster_QoE_numeric'] = gmeet_data['Cluster_QoE_mapped'].map(qoe_mapping)\n\n# Correlation analysis for all labels\ncorrelations = {\n    'Initial_QoE': gmeet_data[qos_features + ['Initial_QoE_numeric']].corr()['Initial_QoE_numeric'],\n    'Cluster_QoE': gmeet_data[qos_features + ['Cluster_QoE_numeric']].corr()['Cluster_QoE_numeric'],\n}\n\nprint(\"Feature-Target Correlation:\")\nfor label, correlation in correlations.items():\n    print(f\"\\nCorrelation with {label}:\")\n    print(correlation)\n\n# Clustering quality metrics for all labels\nmetrics = {}\nfor label, cluster_col in zip(\n    ['Initial_QoE', 'Cluster_QoE'],\n    ['Initial_QoE_numeric', 'Cluster_QoE']\n):\n    silhouette = silhouette_score(qos_scaled, gmeet_data[cluster_col])\n    davies_bouldin = davies_bouldin_score(qos_scaled, gmeet_data[cluster_col])\n    calinski_harabasz = calinski_harabasz_score(qos_scaled, gmeet_data[cluster_col])\n    \n    metrics[label] = {\n        'Silhouette Score': silhouette,\n        'Davies-Bouldin Index': davies_bouldin,\n        'Calinski-Harabasz Index': calinski_harabasz\n    }\n\nprint(\"\\nClustering Quality Metrics:\")\nfor label, metric_values in metrics.items():\n    print(f\"\\nMetrics for {label}:\")\n    for metric_name, value in metric_values.items():\n        print(f\"{metric_name}: {value}\")\n\ngmeet_data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Threshold Based Labels\n\n* ### The threshold-based labels capture some logical relationships (negative correlations), but these correlations are weak overall, which raises concerns about their predictive strength.\n* ### Based on ITU-T standards, these labels align with domain knowledge and logical QoS relationships (negative correlations with impairments).\n* ### Correlations with QoS features are weak, indicating limited predictive power.\n* ### Clustering metrics reveal poorly separated clusters, which may hinder model performance.\n\n### Clustering (K-Means) Based Labels\n\n* ### Relies on clustering assumptions (e.g., distance-based metrics), which might not align with domain-specific QoE relationships.\n* ### Higher clustering quality scores (Silhouette, Davies-Bouldin, and Calinski-Harabasz), indicating better-defined clusters.\n* ### QoS features were standardized before clustering, so the clustering process doesn't explicitly account for the directional relationship between metrics and QoE.","metadata":{}},{"cell_type":"markdown","source":"##  Consolidating Results and Selecting the Best Method","metadata":{}},{"cell_type":"markdown","source":"### Using both Initial_QoE & Cluster_QoE as labels as seperate datasets.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Dataset with Threshold-Based Labels\ndataset_threshold = gmeet_data.copy()\ndataset_threshold['Target'] = gmeet_data['Initial_QoE']\n\n# Drop other unwanted columns for Threshold-Based dataset\ndataset_threshold = dataset_threshold.drop(\n    columns=['Initial_QoE_numeric', 'Cluster_QoE', 'Cluster_QoE_mapped','Final_QoE', 'Final_QoE_numeric','Cluster_QoE_numeric','Initial_QoE'],\n    errors='ignore'\n)\n\n# Save the Threshold-Based dataset\ndataset_threshold.to_csv('/kaggle/working/gmeet_480p_threshold.csv', index=False)\n\n# Dataset with Clustering-Based Labels\ndataset_clustering = gmeet_data.copy()\ndataset_clustering['Target'] = gmeet_data['Cluster_QoE_mapped']\n\n# Drop other unwanted columns for Clustering-Based dataset\ndataset_clustering = dataset_clustering.drop(\n    columns=['Initial_QoE', 'Initial_QoE_numeric', 'Final_QoE', 'Final_QoE_numeric','Cluster_QoE','Cluster_QoE_mapped','Cluster_QoE_numeric'],\n    errors='ignore'\n)\n\n# Save the Clustering-Based dataset\ndataset_clustering.to_csv('/kaggle/working/gmeet_480p_cluster.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Finalizing & Saving Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Read the Threshold-Based dataset\ndataset_threshold = pd.read_csv('/kaggle/input/gmeet-label/gmeet_480p_threshold.csv')\nprint(\"Threshold-Based Dataset Sample:\")\nprint(dataset_threshold.head())\n\n# Display target counts for Threshold-Based dataset\nprint(\"\\nClass Distribution in Threshold-Based Dataset:\")\nprint(dataset_threshold['Target'].value_counts())\n\n# Read the Clustering-Based dataset\ndataset_clustering = pd.read_csv('/kaggle/input/gmeet-label/gmeet_480p_cluster.csv')\nprint(\"\\nClustering-Based Dataset Sample:\")\nprint(dataset_clustering.head())\n\n# Display target counts for Clustering-Based dataset\nprint(\"\\nClass Distribution in Clustering-Based Dataset:\")\nprint(dataset_clustering['Target'].value_counts())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# âš™ï¸| Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## Initial Split (Train-Validation-Test Split)","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Load the dataset\ngmeet_data = pd.read_csv('/kaggle/input/gmeet-label/gmeet_480p_threshold.csv')\n\n# Split the dataset into train, validation, and test sets (stratified)\ntrain_data, test_data = train_test_split(\n    gmeet_data, test_size=0.2, random_state=42, stratify=gmeet_data['Target']\n)\ntrain_data, val_data = train_test_split(\n    train_data, test_size=0.25, random_state=42, stratify=train_data['Target']\n)  # 20% of original data will be validation\n\n# Confirm distributions\nprint(\"Training Set Class Distribution:\\n\", train_data['Target'].value_counts())\nprint(\"\\nValidation Set Class Distribution:\\n\", val_data['Target'].value_counts())\nprint(\"\\nTesting Set Class Distribution:\\n\", test_data['Target'].value_counts())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Outlier Detection and Handling","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Function to detect and handle outliers on training data only\ndef calculate_iqr_bounds(df, feature):\n    \"\"\"Calculate IQR bounds for outlier detection.\"\"\"\n    Q1 = df[feature].quantile(0.25)\n    Q3 = df[feature].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    return lower_bound, upper_bound\n\ndef handle_outliers(df, feature, lower_bound, upper_bound):\n    \"\"\"Replace outliers with the median value.\"\"\"\n    median = df[feature].median()\n    df.loc[(df[feature] < lower_bound) | (df[feature] > upper_bound), feature] = median\n\n# Handle outliers for training set and propagate bounds to validation and test sets\nfor feature in ['Rx packet loss (percent)', 'Tx packet loss (percent)', \n                'RTT (ping) (ms)', 'Rx instant jitter (ms)', 'Tx instant jitter (ms)']:\n    # Calculate IQR bounds on the training set\n    lower_bound, upper_bound = calculate_iqr_bounds(train_data, feature)\n    \n    # Handle outliers in the training set\n    handle_outliers(train_data, feature, lower_bound, upper_bound)\n    \n    # Apply the same bounds to validation and test sets\n    median = train_data[feature].median()\n    val_data.loc[(val_data[feature] < lower_bound) | (val_data[feature] > upper_bound), feature] = median\n    test_data.loc[(test_data[feature] < lower_bound) | (test_data[feature] > upper_bound), feature] = median\n\n# Verify the data after outlier handling\nprint(\"Training Data Summary:\\n\", train_data.describe())\nprint(\"\\nValidation Data Summary:\\n\", val_data.describe())\nprint(\"\\nTest Data Summary:\\n\", test_data.describe())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Extreme values for all numerical features have been mitigated, as evidenced by the statistics:\n### Features like Rx packet loss (percent) and Tx instant jitter (ms) now have reduced ranges, with their max values appearing more consistent with typical QoS conditions.\n\n### The mean and std (standard deviation) values for all features suggest a more normalized and less skewed dataset. This implies that the replacement of outliers (e.g., with median values) has successfully reduced distortions in the data.","metadata":{}},{"cell_type":"markdown","source":"## Feature Importance and Selection","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import mutual_info_classif\n\n# Feature Importance using Random Forest (on training set)\nqos_features = [\n    'Rx packet loss (percent)', 'Tx packet loss (percent)', \n    'RTT (ping) (ms)', 'Rx instant jitter (ms)', 'Tx instant jitter (ms)',\n    'Download (Rx) bandwidth (Mbps)', 'Upload (Tx) bandwidth (Mbps)'\n]\ntarget = 'Target'\n\nrf_model = RandomForestClassifier(random_state=42)\nrf_model.fit(train_data[qos_features], train_data[target])\nimportances = pd.DataFrame({'Feature': qos_features, 'Importance': rf_model.feature_importances_})\nprint(\"Feature Importances:\\n\", importances.sort_values(by='Importance', ascending=False))\n\n# Mutual Information\nmi_scores = mutual_info_classif(train_data[qos_features], train_data[target])\nmi_df = pd.DataFrame({'Feature': qos_features, 'Mutual Information': mi_scores})\nprint(\"\\nMutual Information Scores:\\n\", mi_df.sort_values(by='Mutual Information', ascending=False))\n\n# Select features\nselected_features = list(set(\n    importances[importances['Importance'] > 0.09]['Feature'].tolist() + \n    mi_df[mi_df['Mutual Information'] > 0.05]['Feature'].tolist()\n))\n\nprint(\"\\nSelected Features for Modeling:\\n\", selected_features)\n\n# Update datasets with selected features only\ntrain_data = train_data[selected_features + [target]]\nval_data = val_data[selected_features + [target]]\ntest_data = test_data[selected_features + [target]]\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### Tx instant jitter (ms) (31.5%) and Tx packet loss (percent) (23.9%) are the most important features based on Random Forest, with strong predictive power.\n### Rx instant jitter (ms) (18.6%) and Upload (Tx) bandwidth (Mbps) (9.6%) also show significant contributions.\n### Rx instant jitter (ms) (18.2%) and Tx instant jitter (ms) (15.3%) have the strongest dependency with the target, while features like Rx packet loss (percent) contribute almost no information.\n\n### Retained features: Upload (Tx) bandwidth (Mbps), RTT (ping) (ms), Tx packet loss (percent), Rx instant jitter (ms), Tx instant jitter (ms) for further modeling as they show high relevance to the target variable.","metadata":{}},{"cell_type":"markdown","source":"## Feature Scaling (Robust Scaler)","metadata":{}},{"cell_type":"markdown","source":"### Unlike StandardScaler (uses mean and standard deviation) or Min-Max Scaler (uses min and max values), RobustScaler uses median and interquartile range (IQR) for scaling.\n### This makes it less sensitive to outliers, which are common in QoS datasets due to network anomalies.\n\n### It scales the data by centering around the median and adjusting the spread based on the IQR.\n### Ensures that non-outlier data is appropriately scaled without being skewed by extreme values.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler\nimport joblib\n\n# Initialize RobustScaler and fit on training data only\nscaler = RobustScaler()\ntrain_scaled = scaler.fit_transform(train_data[selected_features])\nval_scaled = scaler.transform(val_data[selected_features])\ntest_scaled = scaler.transform(test_data[selected_features])\n\n# Convert scaled data back to DataFrame\ntrain_scaled_df = pd.DataFrame(train_scaled, columns=selected_features)\nval_scaled_df = pd.DataFrame(val_scaled, columns=selected_features)\ntest_scaled_df = pd.DataFrame(test_scaled, columns=selected_features)\n\n# Add target variable back\ntrain_scaled_df[target] = train_data[target].reset_index(drop=True)\nval_scaled_df[target] = val_data[target].reset_index(drop=True)\ntest_scaled_df[target] = test_data[target].reset_index(drop=True)\n\ntrain_scaled_df.to_csv('/kaggle/working/train_scaled_df.csv', index=False)\n\n# Save the scaler\njoblib.dump(scaler, '/kaggle/working/robust-scaler.pkl')\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imbalanced Data Analysis Before Oversampling\n","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Check class distribution in the training set\nclass_counts = train_scaled_df[target].value_counts()\nprint(\"Class Distribution Before Oversampling in Training Set:\\n\", class_counts)\n\n# Visualize feature distributions by class for the training set only\nfor feature in selected_features:\n    plt.figure(figsize=(10, 6))\n    sns.kdeplot(data=train_scaled_df, x=feature, hue=target, fill=True, alpha=0.7)\n    plt.title(f'Feature Distribution by Class: {feature} (Training Set)')\n    plt.xlabel(feature)\n    plt.ylabel(\"Density\")\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* ### Tx Instant Jitter shows a relatively clearer separation between Good and Poor. Good is skewed toward lower RTT values, while Poor leans toward higher RTT. This feature hence seems crucial for distinguishing Good vs. Poor classes.\n\n\n* ### Features like Upload (Tx) Bandwidth and Tx Instant Jitter are more likely to be significant in separating the classes.\n* ### However, overlap among classes in several features (e.g., Tx Packet Loss & RTT) suggests challenges in class distinction and highlights the need for balancing or using advanced classifiers.\n* ### The small density of Fair class implies it may be harder to learn during modeling, emphasizing the need for careful oversampling (like ADASYN) or using class weighting in models.\n\n\n* ### **These visualizations confirm the need for oversampling techniques (e.g., ADASYN) to balance the Fair class while retaining the distributions of Good and Poor.**\n* ### **The separability of features guides feature importance during model evaluation. Features with better class distinction (e.g., RTT, Jitter) should be prioritized in model training.**\n","metadata":{}},{"cell_type":"markdown","source":"## Oversampling with ADASYN","metadata":{}},{"cell_type":"markdown","source":"### SMOTE generates synthetic samples for the minority class by interpolating between nearest neighbors, assuming uniform distribution across the class. ADASYN, on the other hand, considers the density of data points within the minority class:\n\n### * It generates more samples for minority class instances that are harder to classify (closer to the decision boundary or sparsely distributed).\n### * It generates fewer samples for areas where the minority class is already well-represented.\n### * The Fair class is very sparse and likely unevenly distributed across the feature space. ADASYN adapts to this uneven distribution better than SMOTE.\n### * The sampling_strategy='minority' parameter in oversampling techniques like ADASYN or SMOTE specifies that the minority class (the class with the fewest samples) will be oversampled to balance the dataset.","metadata":{}},{"cell_type":"code","source":"from imblearn.over_sampling import ADASYN\n\n# Separate features and target for oversampling\nX_train = train_scaled_df[selected_features]\ny_train = train_scaled_df[target]\n\nprint(\"Class Distribution in Training Set Before ADASYN:\")\nprint(y_train.value_counts())\n\n\n# Adjust sampling_strategy to ensure ADASYN generates synthetic samples\nadasyn = ADASYN(sampling_strategy='minority', random_state=42)\nX_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)\n\n# Combine resampled data into a DataFrame\ntrain_resampled = pd.DataFrame(X_resampled, columns=selected_features)\ntrain_resampled[target] = y_resampled\n\n# Check the new class distribution\nprint(\"Class Distribution After ADASYN:\\n\", train_resampled[target].value_counts())\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Finalize Datasets for Modeling","metadata":{}},{"cell_type":"code","source":"# Save the final training, validation, and testing datasets\ntrain_resampled.to_csv('/kaggle/working/final_train_dataset.csv', index=False)\nval_scaled_df.to_csv('/kaggle/working/final_validation_dataset.csv', index=False)\ntest_scaled_df.to_csv('/kaggle/working/final_test_dataset.csv', index=False)\n\n# Check the structure of the saved datasets\nprint(\"Training Dataset:\\n\", train_resampled.head())\nprint(\"Validation Dataset:\\n\", val_scaled_df.head())\nprint(\"Testing Dataset:\\n\", test_scaled_df.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ðŸŸ¥ | Model Development","metadata":{}},{"cell_type":"markdown","source":"## Dataset Loading","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load datasets\ntrain_data = pd.read_csv('/kaggle/input/final-datasets/final_train_dataset.csv')\nval_data = pd.read_csv('/kaggle/input/final-datasets/final_validation_dataset.csv')\ntest_data = pd.read_csv('/kaggle/input/final-datasets/final_test_dataset.csv')\n\nprint(f\"Training set size: {train_data.shape}\")\nprint(f\"Validation set size: {val_data.shape}\")\nprint(f\"Test set size: {test_data.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Baseline Models","metadata":{}},{"cell_type":"markdown","source":"### Train simple models to establish baseline performance.\n\n### The baseline model provides a performance reference point for evaluating advanced models.\n### It helps determine whether the advanced models improve upon basic methods like Logistic Regression or Decision Tree.","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Define baseline models\nbaseline_models = {\n    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n    'Decision Tree': DecisionTreeClassifier(random_state=42),\n    'SVM': SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42)\n}\n\n# Initialize the dictionary to store trained models\ntrained_baseline_models = {}\n\n# Train and evaluate baseline models on the validation set\nfor name, model in baseline_models.items():\n    print(f\"\\nTraining and Evaluating: {name}\")\n    model.fit(train_data.drop(columns='Target'), train_data['Target'])  # Train the model\n    trained_baseline_models[name] = model  # Save the trained model\n    \n    # Generate predictions\n    val_preds = model.predict(val_data.drop(columns='Target'))\n    \n    # Classification Report\n    print(f\"{name} Performance on Validation Set:\\n\")\n    print(classification_report(val_data['Target'], val_preds))\n    \n    # Confusion Matrix\n    plt.figure(figsize=(6, 4))\n    sns.heatmap(confusion_matrix(val_data['Target'], val_preds), annot=True, fmt='d', cmap='Blues')\n    plt.title(f\"Confusion Matrix: {name}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Actual\")\n    plt.show()\n\n# Accessing the trained Decision Tree model\ndt_model = trained_baseline_models['Decision Tree']\nprint(\"Trained Decision Tree model retrieved successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Logistic Regression (Overall Accuracy 0.69)\n\n* ### Fair Class has poor performance with a precision of 0.17 and a recall of 0.60.This indicates that when the model predicts \"Fair,\" it is often incorrect, but it captures 60% of actual \"Fair\" instances.\n* ### For Good class, Decent performance with precision and recall around 0.77-0.78.\n* ### Slightly better performance than \"Fair,\" but recall (0.60) indicates that the model misses a significant number of \"Poor\" instances.\n\n\n### Decision Tree (Overall Accuracy 0.90)\n\n\n* ### Fair class, Slightly better than Logistic Regression but still poor, with precision of 0.25 and recall of 0.40.\n* ### Good class, High performance with both precision and recall around 0.94-0.96.\n* ### Strong performance with precision of 0.91 and recall of 0.88 for Poor class.\n* ### Macro Average scores for Precision, Recall & F1 between 0.70 & 0.74, much better than Logisitic Regression & SVM.\n\n\n\n### Support Vector Machine (Overall Accuracy 0.78)\n\n\n* ### Best performance on \"Fair\" compared to other models, with precision of 0.27 and recall of 0.80.\n* ### On Poor Class, slightly lower performance than Decision Tree, with precision of 0.91 and recall of 0.67.\n\n\n### **Decision Tree has the best overall performance with an accuracy of 90%, strong metrics for \"Good\" and \"Poor\" classes, and slightly improved handling of the \"Fair\" class compared to Logistic Regression & SVM. Although the predictions and scores for the \"Fair\" class is still a challenge.**","metadata":{}},{"cell_type":"markdown","source":"## Stacking Ensemble","metadata":{}},{"cell_type":"markdown","source":"### Base Models Used : Random Forest, Gradient Boosting, SVM\n### Meta Models Used : Logistic Regression.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Define base models\nbase_models = [\n    ('Random Forest', RandomForestClassifier(random_state=42)),\n    ('Gradient Boosting', GradientBoostingClassifier(random_state=42)),\n    ('SVM', SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42))\n]\n\n# Define meta model\nmeta_model = LogisticRegression(max_iter=1000, random_state=42)\n\n# Define stacking ensemble\nstacking_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, cv=5)\n\n# Train stacking model on training set\nstacking_model.fit(train_data.drop(columns='Target'), train_data['Target'])\n\n# Evaluate on validation set\nval_preds = stacking_model.predict(val_data.drop(columns='Target'))\nprint(\"Stacking Ensemble Performance on Validation Set:\\n\")\nprint(classification_report(val_data['Target'], val_preds))\n\n# Confusion Matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(confusion_matrix(val_data['Target'], val_preds), annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix: Stacking Ensemble\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### The stacking ensemble achieved an impressive accuracy of 94% on the validation set, which is significantly better than the baseline models.\n\n### Class-Wise Performance:\n\n### Fair Class (0):\n### Precision: 0.43, indicating the model's predictions for \"Fair\" are often incorrect.\n### Recall: 0.60, showing it captures 60% of actual \"Fair\" instances, better than any baseline model.\n### F1-Score: 0.50, demonstrating some improvement in handling this underrepresented class.\n\n### Good Class (1):\n### Precision: 0.94, Recall: 0.97, and F1-Score: 0.96, indicating excellent performance for the majority class.\n### Poor Class (2):\n### Precision: 1.00, Recall: 0.93, and F1-Score: 0.97, suggesting outstanding performance, though it misses a few instances.\n\n### Macro Average : Precision: 0.79, Recall: 0.83, and F1-Score: 0.81, indicating improved balance compared to baseline models. Macro Average provides a much more realistic view than Weighted Average which takes into account label ratios of the dataset.\n\n### **The stacking ensemble outperforms all baseline models and serves as a robust solution for this dataset.**\n### **While the ensemble performs exceptionally well on the \"Good\" and \"Poor\" classes, additional attention is needed for the \"Fair\" class.**","metadata":{}},{"cell_type":"markdown","source":"# ðŸ”§| Hyperparameter Tuning","metadata":{}},{"cell_type":"markdown","source":"## Setup for Base Model Tuning","metadata":{}},{"cell_type":"markdown","source":"### This step involves using GridSearchCV and RandomizedSearchCV with Stratified K-Fold Cross-Validation to optimize the hyperparameters of the base models (Random Forest, Gradient Boosting, and SVM). The goal is to identify the best-performing parameter combinations while ensuring the evaluation is unbiased across stratified folds of the training data.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, roc_auc_score\n\n# Define parameter grids for base models\nrf_param_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngb_param_grid = {\n    'learning_rate': [0.01, 0.1, 0.2],\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 5, 10]\n}\n\nsvm_param_grid = {\n    'C': [0.1, 1, 10],\n    'gamma': [0.001, 0.01, 0.1],\n    'kernel': ['rbf', 'linear']\n}\n\n# Initialize models\nrf_model = RandomForestClassifier(random_state=42)\ngb_model = GradientBoostingClassifier(random_state=42)\nsvm_model = SVC(probability=True, random_state=42)\n\n# Define cross-validation strategy\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Randomized Search for Base Models\n","metadata":{}},{"cell_type":"markdown","source":"### This code performs hyperparameter tuning for the base models using RandomizedSearchCV (for Random Forest and Gradient Boosting) and GridSearchCV (for SVM). \n\n### It identifies the best hyperparameter combinations by evaluating performance across Stratified K-Fold Cross-Validation on the training set, ensuring robust model optimization while mitigating overfitting risks.","metadata":{}},{"cell_type":"code","source":"# Randomized Search for Random Forest\nrf_random_search = RandomizedSearchCV(\n    estimator=rf_model,\n    param_distributions=rf_param_grid,\n    n_iter=20,\n    scoring='accuracy',\n    cv=cv,\n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\nrf_random_search.fit(train_scaled_df[selected_features], train_scaled_df['Target'])\nprint(\"Best Random Forest Parameters:\", rf_random_search.best_params_)\n\n# Randomized Search for Gradient Boosting\ngb_random_search = RandomizedSearchCV(\n    estimator=gb_model,\n    param_distributions=gb_param_grid,\n    n_iter=20,\n    scoring='accuracy',\n    cv=cv,\n    verbose=2,\n    random_state=42,\n    n_jobs=-1\n)\ngb_random_search.fit(train_scaled_df[selected_features], train_scaled_df['Target'])\nprint(\"Best Gradient Boosting Parameters:\", gb_random_search.best_params_)\n\n# Grid Search for SVM\nsvm_grid_search = GridSearchCV(\n    estimator=svm_model,\n    param_grid=svm_param_grid,\n    scoring='accuracy',\n    cv=cv,\n    verbose=2,\n    n_jobs=-1\n)\nsvm_grid_search.fit(train_scaled_df[selected_features], train_scaled_df['Target'])\nprint(\"Best SVM Parameters:\", svm_grid_search.best_params_)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Best Random Forest Parameters: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_depth': 30}\n### Best Gradient Boosting Parameters: {'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.1}\n### Best SVM Parameters: {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n","metadata":{}},{"cell_type":"markdown","source":"## Meta Model Tuning","metadata":{}},{"cell_type":"markdown","source":"### Here, we perform hyperparameter tuning for the Logistic Regression meta model using GridSearchCV. \n### The process evaluates combinations of regularization strength (C), penalty (l2), and solver (lbfgs) across Stratified K-Fold Cross-Validation on the training set.","metadata":{}},{"cell_type":"code","source":"# Define parameter grid for Logistic Regression\nlr_param_grid = {\n    'C': [0.1, 1, 10],\n    'penalty': ['l2'],\n    'solver': ['lbfgs']\n}\n\n# Logistic Regression Grid Search\nlr_grid_search = GridSearchCV(\n    estimator=LogisticRegression(random_state=42, max_iter=1000),\n    param_grid=lr_param_grid,\n    scoring='accuracy',\n    cv=cv,\n    verbose=2,\n    n_jobs=-1\n)\nlr_grid_search.fit(train_scaled_df[selected_features], train_scaled_df['Target'])\nprint(\"Best Logistic Regression Parameters:\", lr_grid_search.best_params_)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Best Logistic Regression Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n","metadata":{}},{"cell_type":"markdown","source":"## Build Tuned Stacking Ensemble","metadata":{}},{"cell_type":"markdown","source":"### A stacking ensemble is created using the best-tuned estimators (Random Forest, Gradient Boosting, and SVM) and a Logistic Regression model as the meta-model.\n\n### The tuned stacking model is trained on the training dataset (train_scaled_df) with the selected features.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\n\n# Define tuned base models and meta model\nbase_models_tuned = [\n    ('Random Forest', rf_random_search.best_estimator_),\n    ('Gradient Boosting', gb_random_search.best_estimator_),\n    ('SVM', svm_grid_search.best_estimator_)\n]\nmeta_model_tuned = lr_grid_search.best_estimator_\n\n# Define stacking ensemble\nstacking_model_tuned = StackingClassifier(\n    estimators=base_models_tuned,\n    final_estimator=meta_model_tuned,\n    cv=cv\n)\n\n# Train stacking ensemble\nstacking_model_tuned.fit(train_scaled_df[selected_features], train_scaled_df['Target'])\n\n\nprint(\"UnTuned Stacking Ensemble Performance on Validation Set:\\n\")\nprint(classification_report(val_scaled_df['Target'], val_preds))\n\nplt.figure(figsize=(6, 4))\nsns.heatmap(confusion_matrix(val_data['Target'], val_preds), annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix: Stacking Ensemble Untuned\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n# Evaluate stacking ensemble on validation set\ny_pred_stack = stacking_model_tuned.predict(val_scaled_df[selected_features])\nprint(\"Tuned Stacking Ensemble Performance on Validation Set:\\n\")\nprint(classification_report(val_scaled_df['Target'], y_pred_stack))\n\nplt.figure(figsize=(6, 4))\nsns.heatmap(confusion_matrix(val_data['Target'], y_pred_stack), annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion Matrix: Stacking Ensemble Tuned\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Untuned Stacking Ensemble Performance**\n\n### -   Accuracy: 94%\n### -   Macro Average F1-Score: 0.81\n### -   The untuned ensemble already performs well but shows some inconsistency in precision and recall for the minority class (\"Fair\")\n\n### **Tuned Stacking Ensemble Performance**\n\n### -   Accuracy: 96%\n### -   Macro Average F1-Score: 0.87\n### -   Precision for the \"Fair\" class improved significantly (from 43% to 75%).\n### -   The f1 score  for the \"Fair\" class improved (from 50% to 67%), indicating better handling of the minority class.\n### -   Slight improvements in \"Good\" and \"Poor\" class predictions, maintaining high precision and recall.\n\n### **Fine-tuning the base and meta models optimized the ensemble's ability to generalize on unseen validation data.**\n### **Better f1 score for the \"Fair\" class demonstrates that the tuned model is less biased towards majority classes.**","metadata":{}},{"cell_type":"markdown","source":"# ðŸ¤–| Explainable AI","metadata":{}},{"cell_type":"markdown","source":" ## SHAP for Random Forest\n\n","metadata":{}},{"cell_type":"markdown","source":"\n* ### The summary plot shows the global importance of each feature for the Random Forest model.\n* ### The features are ranked from top to bottom based on their average contribution to the predictions across all validation samples.\n* ### The mean(|SHAP value|) on the x-axis indicates the average magnitude of a feature's influence on the model's output.\n\n","metadata":{}},{"cell_type":"markdown","source":"### Summary Plot","metadata":{}},{"cell_type":"code","source":"import shap\nimport matplotlib.pyplot as plt\n\nfor idx, class_label in enumerate(rf_random_search.best_estimator_.classes_):\n    print(f\"Class Index: {idx}, Class Label: {class_label}\")\n\n# Initialize SHAP explainer for Random Forest\nrf_explainer = shap.TreeExplainer(rf_random_search.best_estimator_)\n\n# Generate SHAP values for the validation set\nrf_shap_values = rf_explainer.shap_values(val_scaled_df[selected_features])\n\n# Global Feature Importance for Random Forest\nshap.summary_plot(rf_shap_values, val_scaled_df[selected_features], feature_names=selected_features)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Global Feature Importance for Random Forest\n### Top Influential Features:\n\n### -   Tx instant jitter (ms) has the highest average impact on model predictions for all three classes (Class 0, Class 1, Class 2).\n### -   Tx packet loss (percent) and **Rx instant jitter (ms)** also significantly affect predictions.\n\n### **The inference is on improving the key QoS parameters, such as reducing jitter and packet loss, as they have the most significant impact on predicting the quality of the video conferencing service.**","metadata":{}},{"cell_type":"markdown","source":"### Force Plots","metadata":{}},{"cell_type":"markdown","source":"* ### The force plot shows the explanation for a specific prediction (here at instance index 10).\n* ### It highlights how individual features contribute to pushing the prediction away from the base value (model's expected output without any feature input).","metadata":{}},{"cell_type":"code","source":"import shap\nshap.initjs()  # Initialize JavaScript for SHAP visualizations\n\nfor idx, class_label in enumerate(rf_random_search.best_estimator_.classes_):\n    print(f\"Class Index: {idx}, Class Label: {class_label}\")\n\n# Local Explanation for a Single Instance\ninstance_idx = 10  # Example instance index\n\n# Extract predicted class and probabilities\npredicted_class = rf_random_search.best_estimator_.predict(val_scaled_df[selected_features])[instance_idx]\npredicted_probabilities = rf_random_search.best_estimator_.predict_proba(val_scaled_df[selected_features])[instance_idx]\n\nprint(f\"Predicted Class: {predicted_class}\")\nprint(f\"Predicted Probabilities: {predicted_probabilities}\")\n\n# Render force plots for each class\nfor class_idx, class_name in enumerate(['Class 0', 'Class 1', 'Class 2']):  # Adjust class names\n    print(f\"\\nForce Plot for {class_name} (Class Index: {class_idx}):\")\n    shap.force_plot(\n        rf_explainer.expected_value[class_idx],  # Base value for the class\n        rf_shap_values[class_idx][instance_idx],  # SHAP values for the instance and class\n        val_scaled_df.iloc[instance_idx][selected_features], matplotlib=True  # Feature values for the instance\n    )\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n\n### Force Plot at instance - index 10\n### Interpretation:\n\n### -   Base Value: The model's average output value for all instances in the dataset.\n### -   f(x): The predicted probability (or log-odds) for the given instance.\n### -   Red Arrows:\n###     -   Features with **positive contributions** that push the prediction towards the predicted class.\n###     -   Example: **Rx instant jitter (ms)** and **Tx instant jitter (ms)** are pushing the prediction strongly in the direction of the predicted class.\n### -   Blue Arrows:\n###     -   Features with negative contributions that oppose the prediction.","metadata":{}},{"cell_type":"markdown","source":"### Dependence Plot ","metadata":{}},{"cell_type":"markdown","source":"### The SHAP Dependence Plot visualizes the relationship between a specific feature (e.g., Tx packet loss (percent)) and its contribution to predictions for a given class (e.g., Poor).\n","metadata":{}},{"cell_type":"code","source":"for idx, class_label in enumerate(rf_random_search.best_estimator_.classes_):\n    print(f\"Class Index: {idx}, Class Label: {class_label}\")\n\ndef check_shap_class(idx, shap_values, class_labels):\n    print(f\"SHAP values correspond to Class: {class_labels[idx]}\")\ncheck_shap_class(2, rf_shap_values, rf_random_search.best_estimator_.classes_)\n\n\nshap.dependence_plot(\n    'Tx packet loss (percent)',\n    rf_shap_values[2],  # Class-specific SHAP values for class Poor (index 2 for other classes)\n    val_scaled_df[selected_features]\n)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"###    **Feature on the X-axis**:\n\n###       The x-axis represents the values of the feature `Tx packet loss (percent)`. This is the input feature for which the relationship with the model's output (SHAP value) is being analyzed.-   \n\n### **SHAP Value on the Y-axis**:\n\n###       The y-axis represents the SHAP values for the feature `Tx packet loss (percent)`. A positive SHAP value indicates that the feature pushes the prediction towards the respective class, while a negative value pushes it away.-   \n\n### **Color Coding (Additional Feature)**:\n\n###      The color of the points represents the value of another feature, `Tx instant jitter (ms)`, to show interaction effects.\n###      Higher `Tx instant jitter (ms)` values (darker pink) might amplify the impact of `Tx packet loss (percent)` on the prediction.-   \n\n### **Insights from the Plot**:\n\n\n* ###       When `Tx packet loss (percent)` is near `-1`, the SHAP value is mostly negative, meaning this feature has a negative impact on the prediction for the specific class (`Poor`).\n* ###       As `Tx packet loss (percent)` increases from 0, the SHAP values become more positive, indicating that higher packet loss increases the likelihood of predicting the class (`Poor`).\n* ###       The interaction with `Tx instant jitter (ms)` is visible through the color variations. For example, at the same `Tx packet loss (percent)` value, points with higher `Tx instant jitter (ms)` (pink) have different SHAP values than those with lower jitter (blue).\n","metadata":{}},{"cell_type":"markdown","source":"### Decision Plot","metadata":{}},{"cell_type":"markdown","source":"### The SHAP Decision Plot illustrates how SHAP values for all features cumulatively impact the prediction across all instances for each class (e.g., Fair, Good, Poor).\n","metadata":{}},{"cell_type":"code","source":"# Generate SHAP Decision Plots for all instances for each class\nfor class_idx, class_label in enumerate(['Fair', 'Good', 'Poor']):\n    print(f\"SHAP Decision Plot for Class {class_label} (Class Index: {class_idx}):\")\n    shap.decision_plot(\n        rf_explainer.expected_value[class_idx],  # Base value for the class\n        rf_shap_values[class_idx],  # SHAP values for all instances for the class\n        feature_names=selected_features,\n        title=f\"Decision Plot for Class {class_label} (All Instances)\",\n        show=True\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Model Output Value:\n\n\n* ### The x-axis represents how much each feature contributes to the final output for a specific class.\n* ### Values closer to 1 indicate a higher likelihood of being classified as the respective class, while negative or closer to 0 suggests a lower likelihood.\n* ### Color Bar: Indicates the feature value (blue for lower values, pink for higher values).\n\n\n### **Class Fair (Class Index: 0)**\n\n###   The plot shows the contribution of each feature to the model's output value for the \"Fair\" class across all instances.\n\n###       **Rx instant jitter (ms)** and **Tx packet loss (percent)** appear to have the most significant impact in pushing the model output value toward or away from classifying as \"Fair.\"\n###       Negative contributions (left of 0) reduce the likelihood of being classified as \"Fair,\" while positive contributions (right of 0) increase it.\n\n### **Class Good (Class Index: 1)**\n\n###   This plot represents contributions to the \"Good\" class.\n\n###       Features like **Tx instant jitter (ms)** and **Tx packet loss (percent)** strongly influence predictions for \"Good.\"\n###       Some instances have **Tx instant jitter (ms)** contributing positively, moving the model's output value toward the \"Good\" class, while others push it away.\n\n### **Class Poor (Class Index: 2)**\n\n###   This plot shows contributions to the \"Poor\" class.\n\n###       **Tx instant jitter (ms)** and **Tx packet loss (percent)** are significant factors for predicting \"Poor.\"\n###       Many instances are influenced heavily by these features, pushing the output value positively (toward classifying as \"Poor\").\n\n","metadata":{}},{"cell_type":"markdown","source":"### **Insights**\n\n### **Feature Importance**:\n\n###   Across all classes, **Tx instant jitter (ms)** and **Tx packet loss (percent)** play a significant role in shaping predictions.\n###   The direction and magnitude of influence vary based on the class.\n\n### **Divergent Trends**:\n\n###   The same feature can have opposing effects across different classes, highlighting its nuanced role in decision-making.","metadata":{}},{"cell_type":"markdown","source":"##  LIME for SVM","metadata":{}},{"cell_type":"markdown","source":"\n* ### LIME explains the predictions for an individual instance by generating perturbations (slight variations) of the instance and observing how the model's predictions change.\n* ### To simulate the data distribution around the instance, LIME uses the training data as a reference to build its local neighborhood.\n* ### This ensures that the perturbations are realistic and consistent with the distribution the model was trained on.\n","metadata":{}},{"cell_type":"markdown","source":"### Initialize LIME Tabular Explainer","metadata":{}},{"cell_type":"markdown","source":"### Initialize LIME Explainer: The LimeTabularExplainer is initialized with the training data, feature names, and class names to provide explanations for classification predictions.\n### Classification Mode: The mode='classification' parameter ensures the explainer is configured for classification tasks, mapping features to the class probabilities.","metadata":{}},{"cell_type":"code","source":"import lime\nfrom lime.lime_tabular import LimeTabularExplainer\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Initialize the LIME explainer\nlime_explainer = LimeTabularExplainer(\n    training_data=train_scaled_df[selected_features].values,  # Use training data\n    training_labels=train_scaled_df['Target'].values,\n    feature_names=selected_features,\n    class_names=['Fair', 'Good', 'Poor'],  # Provide the class names\n    mode='classification'\n)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Explain an Instance & Visualization","metadata":{}},{"cell_type":"markdown","source":"### explain_instance explains the prediction for a specific validation instance using the predict_proba method of the tuned SVM, highlighting the top 5 contributing features.","metadata":{}},{"cell_type":"code","source":"# Choose a specific instance index from the validation set\ninstance_idx = 40  # Example instance index\ninstance = val_scaled_df[selected_features].iloc[instance_idx].values.reshape(1, -1)\n\n# Generate an explanation for the instance\nlime_explanation = lime_explainer.explain_instance(\n    data_row=instance.flatten(),  # Flatten the instance for input\n    predict_fn=svm_grid_search.best_estimator_.predict_proba,  # Use the predict_proba method of the tuned SVM\n    num_features=5  # Limit to the top 5 contributing features\n)\n\n# Display the explanation as a table\nlime_explanation.show_in_notebook()\n\n# Save the explanation as an HTML file\nlime_explanation.save_to_file('/kaggle/working/lime_explanation.html')\n\n# Visualize the explanation as a bar plot\nlime_explanation.as_pyplot_figure()\nplt.title(f'LIME Explanation for Instance {instance_idx}')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Prediction Summary (Top Left)\n\n\n* ### Fair (0.00), Good (0.94), Poor (0.06)\n* ### The model predicts \"Poor\" with the highest probability (88%).\n\n### Explanation of the Prediction (Bottom Bar Plot)\n\n### This bar plot shows the top 5 features and their impact (positive or negative) on the predicted class:\n### The low value of Tx Instant Jitter contributes significantly to the instance being classified as Good.\n### The high RTT (ping) value pulls the prediction slightly away from Good, indicating it's less favorable for this class.\n\n\n### **The strongest contributor is Tx Instant Jitter (ms) â‰¤ -0.51, which heavily supports the classification of Good.**\n### **Despite negative influences like RTT (ping) (ms) > 0.47 and Tx Packet Loss (percent) > 0.45, the positive contributions outweigh them, leading to a confident classification.**\n### **The top green bar (Tx Instant Jitter (ms) â‰¤ -0.51) visually dominates, showing it is the primary reason for the Good classification.**","metadata":{}},{"cell_type":"markdown","source":"# âœ”ï¸| Final Model Evaluation","metadata":{}},{"cell_type":"markdown","source":"## Evaluating Performance on the Test Set","metadata":{}},{"cell_type":"markdown","source":"### Generate Predictions on Test Set","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, log_loss\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Generate predictions\ny_test = test_scaled_df['Target']\nX_test = test_scaled_df[selected_features]\n\ny_pred_test = stacking_model_tuned.predict(X_test)\ny_pred_proba_test = stacking_model_tuned.predict_proba(X_test)\n\n# Display predictions\nprint(f\"Predicted Labels: {y_pred_test[:10]}\")\nprint(f\"Predicted Probabilities (first 10 rows):\\n{y_pred_proba_test[:10]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Calculate Performance Metrics & Confusion Matrix","metadata":{}},{"cell_type":"markdown","source":"### Here, we calculate and display the classification report with precision, recall, F1-score, and support for each class along with a Confusion Matrix.\n### Also Log Loss & Macro ROC-AUC is calculated.\n\n* ### Log Loss focuses on the calibration of probabilities. It rewards well-calibrated predictions and penalizes incorrect predictions with high confidence.\n* ### ROC-AUC evaluates the model's discrimination ability â€” its ability to rank true positives higher than false positives, regardless of probability calibration.\n","metadata":{}},{"cell_type":"code","source":"# Classification report\nprint(\"Classification Report on Test Data:\")\nprint(classification_report(y_test, y_pred_test, target_names=['Fair', 'Good', 'Poor']))\n\n# Log Loss\nlogloss = log_loss(y_test, y_pred_proba_test)\nprint(f\"Log Loss: {logloss}\")\n\n# ROC-AUC (macro and per class)\nroc_auc_macro = roc_auc_score(y_test, y_pred_proba_test, multi_class='ovr', average='macro')\nprint(f\"Macro ROC-AUC: {roc_auc_macro}\")\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred_test)\n\n# Heatmap Visualization\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Fair', 'Good', 'Poor'], yticklabels=['Fair', 'Good', 'Poor'])\nplt.title(\"Confusion Matrix: Test Data\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* ### Fair: Precision and recall are 0.83, indicating balanced performance for this class, though based on fewer instances (support: 6).\n* ### Good: Precision and recall are 0.96, showing excellent performance with the highest support (69 instances).\n* ### Poor: Precision and recall are 0.97, indicating strong performance across all metrics with support of 59 instances.\n\n\n### Log Loss: Low value (0.1859), indicating that the model generates well-calibrated probabilities.\n\n### Macro ROC-AUC: 0.98, showing excellent discriminatory ability for all classes in a one-vs-all setup.","metadata":{}},{"cell_type":"markdown","source":"### ROC Curves","metadata":{}},{"cell_type":"markdown","source":"\n* ### The ROC curve is a graphical representation of a classifier's ability to distinguish between classes.\n* ### It plots the True Positive Rate (TPR) (Sensitivity) against the False Positive Rate (FPR) at different thresholds for classification.\n\n\n---- \n\n\n* ### The AUC measures the overall performance of the classifier.\n* ### An AUC of 1.0 indicates perfect classification, while 0.5 indicates no discrimination (equivalent to random guessing).\n\n\n-----\n\n\n* ### In multi-class problems, the ROC curve is computed for each class, treating that class as \"positive\" and all others as \"negative.\"\n* ### This process is repeated for each class, generating a separate ROC curve for each.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\n# Binarize the target labels\ny_test_binary = label_binarize(y_test, classes=['Fair', 'Good', 'Poor'])\n\n# Generate ROC Curves\nplt.figure(figsize=(10, 8))\n\nfor class_idx, class_label in enumerate(['Fair', 'Good', 'Poor']):\n    # Compute False Positive Rate, True Positive Rate, and thresholds\n    fpr, tpr, _ = roc_curve(y_test_binary[:, class_idx], y_pred_proba_test[:, class_idx])\n    # Compute Area Under Curve (AUC)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f\"ROC Curve for {class_label} (AUC = {roc_auc:.2f})\")\n\n# Plot the random guess line\nplt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\nplt.title(\"ROC Curves (One-vs-All)\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* ### A steep curve closer to the top-left corner indicates strong performance, meaning the model achieves high sensitivity with low false positives.\n* ### Each class has an AUC (Area Under the Curve) of 0.99, which indicates near-perfect discrimination capability. The closer the AUC is to 1.0, the better the model is at separating that class from the rest.\n* ### The dashed diagonal line represents a random classifier (AUC = 0.5). Any ROC curve above this line reflects a model better than random guessing.\n","metadata":{}},{"cell_type":"markdown","source":"### Precision-Recall Curves","metadata":{}},{"cell_type":"markdown","source":"### Precision-Recall (PR) curves evaluate the trade-off between precision (positive predictive value) and recall (sensitivity) at different probability thresholds for each class (Fair, Good, Poor).\n\n-----\n\n### Here, we :\n\n* ### Compute precision and recall for each threshold using the binarized true labels (y_test_binary) and predicted probabilities (y_pred_proba_test) for each class.\n\n* ### Calculates the Average Precision (AP), a summary statistic that represents the area under the Precision-Recall curve. Higher AP values indicate better performance.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve, average_precision_score\n\n# Generate Precision-Recall Curves\nplt.figure(figsize=(10, 8))\n\nfor class_idx, class_label in enumerate(['Fair', 'Good', 'Poor']):\n    # Compute Precision, Recall, and thresholds\n    precision, recall, _ = precision_recall_curve(y_test_binary[:, class_idx], y_pred_proba_test[:, class_idx])\n    # Compute Average Precision Score (AP)\n    avg_precision = average_precision_score(y_test_binary[:, class_idx], y_pred_proba_test[:, class_idx])\n    plt.plot(recall, precision, label=f\"Precision-Recall for {class_label} (AP = {avg_precision:.2f})\")\n\nplt.title(\"Precision-Recall Curves (One-vs-All)\")\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n* ### Precision and recall drop more sharply for the Fair class compared to the other two, likely because there are fewer instances of Fair in the dataset (imbalanced class).\n* ### Average Precision (AP) for Fair is 0.88, indicating relatively good but not perfect performance.\n* ### Both Good & Poor classes maintain high precision and recall for most thresholds, resulting in AP values close to 0.99. This suggests the model is very effective at predicting these classes.\n","metadata":{}},{"cell_type":"markdown","source":"# ðŸš…| Save Model & Pipeline ","metadata":{}},{"cell_type":"markdown","source":"## Save the Model","metadata":{}},{"cell_type":"code","source":"import joblib\n\n# Save the tuned stacking ensemble model\nmodel_path = '/kaggle/working/stacking_model_tuned.pkl'\njoblib.dump(stacking_model_tuned, model_path)\nprint(f\"Tuned Stacking Ensemble Model saved at {model_path}\")\n\n# Reload the saved model\nloaded_model = joblib.load(model_path)\n\n# Check if the loaded model is the same as the trained one\nprint(loaded_model)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Save the Preprocessing Pipeline","metadata":{}},{"cell_type":"code","source":"# Save the RobustScaler used for preprocessing\nscaler_path = '/kaggle/working/robust_scaler.pkl'\njoblib.dump(scaler, scaler_path)\nprint(f\"Scaler saved at {scaler_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}